{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch基础概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "科学计算工具包，一般两个核心问题：\n",
    "1.怎么定义数据？torch.Tensor\n",
    "2.怎么对数据操作？torch.autograd.Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何用PyTorch完成实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的代码实现流程：\n",
    "加载、预处理数据集、构建模型、定义损失函数、实现优化算法、迭代训练、加速计算（GPU）、存储模型、构建baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加载、预处理数据集**\n",
    "\n",
    "一旦选定了数据集，就要写一些函数去load数据集，然后presprocess数据集，normalize数据集。（可能是实验中占比最多的地方）\n",
    "因为：每个数据集的格式都不太一样；需要告诉feed data；预处理和正则化的方式多种多样；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vosion和text分别对应于视觉和自然语言处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████▉| 9912320/9912422 [00:27<00:00, 205356.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                        | 0/28881 [00:00<?, ?it/s]\n",
      " 57%|█████████████████████████████████████████▍                               | 16384/28881 [00:01<00:00, 32428.09it/s]\n",
      "32768it [00:01, 30163.75it/s]                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\train-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "  0%|                                                                                      | 0/1648877 [00:00<?, ?it/s]\n",
      "  1%|█                                                                     | 24576/1648877 [00:01<00:16, 100870.16it/s]\n",
      "  2%|█▍                                                                     | 32768/1648877 [00:01<00:25, 62679.13it/s]\n",
      "  2%|█▊                                                                     | 40960/1648877 [00:01<00:32, 49603.41it/s]\n",
      "  3%|██▍                                                                    | 57344/1648877 [00:01<00:30, 53022.51it/s]\n",
      "  4%|██▊                                                                    | 65536/1648877 [00:02<00:34, 45586.92it/s]\n",
      "  5%|███▌                                                                   | 81920/1648877 [00:02<00:31, 50062.64it/s]\n",
      "  6%|████▏                                                                  | 98304/1648877 [00:02<00:28, 55108.05it/s]\n",
      "  6%|████▌                                                                 | 106496/1648877 [00:02<00:32, 47381.56it/s]\n",
      "  7%|█████▏                                                                | 122880/1648877 [00:02<00:29, 51756.84it/s]\n",
      "  8%|█████▉                                                                | 139264/1648877 [00:03<00:27, 55110.73it/s]\n",
      "  9%|██████▌                                                               | 155648/1648877 [00:03<00:25, 58842.10it/s]\n",
      " 10%|███████▎                                                              | 172032/1648877 [00:03<00:23, 61831.72it/s]\n",
      " 11%|███████▉                                                              | 188416/1648877 [00:03<00:23, 62796.48it/s]\n",
      " 12%|████████▋                                                             | 204800/1648877 [00:04<00:22, 65001.42it/s]\n",
      " 13%|█████████▍                                                            | 221184/1648877 [00:04<00:21, 65620.38it/s]\n",
      " 14%|██████████                                                            | 237568/1648877 [00:04<00:21, 66061.20it/s]\n",
      " 16%|███████████▏                                                          | 262144/1648877 [00:04<00:19, 71120.10it/s]\n",
      " 17%|███████████▊                                                          | 278528/1648877 [00:05<00:18, 73215.96it/s]\n",
      " 18%|████████████▊                                                         | 303104/1648877 [00:05<00:16, 79799.94it/s]\n",
      " 20%|█████████████▉                                                        | 327680/1648877 [00:05<00:15, 85067.86it/s]\n",
      " 21%|██████████████▌                                                       | 344064/1648877 [00:05<00:17, 74647.63it/s]\n",
      " 22%|███████████████▋                                                      | 368640/1648877 [00:06<00:14, 86961.23it/s]\n",
      " 24%|████████████████▋                                                     | 393216/1648877 [00:06<00:14, 88784.87it/s]\n",
      " 25%|█████████████████▋                                                    | 417792/1648877 [00:06<00:13, 92095.40it/s]\n",
      " 27%|██████████████████▊                                                   | 442368/1648877 [00:06<00:12, 96370.97it/s]\n",
      " 28%|███████████████████▏                                                 | 458752/1648877 [00:06<00:11, 105557.61it/s]\n",
      " 29%|███████████████████▉                                                 | 475136/1648877 [00:07<00:11, 102899.21it/s]\n",
      " 30%|████████████████████▊                                                 | 491520/1648877 [00:07<00:12, 94711.22it/s]\n",
      " 31%|█████████████████████▌                                                | 507904/1648877 [00:07<00:14, 76809.25it/s]\n",
      " 33%|██████████████████████▉                                               | 540672/1648877 [00:07<00:11, 94618.94it/s]\n",
      " 34%|███████████████████████▉                                              | 565248/1648877 [00:08<00:11, 97012.23it/s]\n",
      " 35%|████████████████████████▋                                             | 581632/1648877 [00:08<00:11, 93230.56it/s]\n",
      " 36%|█████████████████████████                                            | 598016/1648877 [00:08<00:10, 100436.69it/s]\n",
      " 37%|█████████████████████████▋                                           | 614400/1648877 [00:08<00:10, 102421.31it/s]\n",
      " 38%|██████████████████████████▊                                           | 630784/1648877 [00:08<00:10, 97556.06it/s]\n",
      " 39%|███████████████████████████                                          | 647168/1648877 [00:08<00:09, 105530.91it/s]\n",
      " 40%|███████████████████████████▊                                         | 663552/1648877 [00:08<00:09, 102551.89it/s]\n",
      " 41%|████████████████████████████▍                                        | 679936/1648877 [00:09<00:08, 108054.52it/s]\n",
      " 42%|█████████████████████████████▏                                       | 696320/1648877 [00:09<00:08, 114909.08it/s]\n",
      " 43%|█████████████████████████████▊                                       | 712704/1648877 [00:09<00:09, 100611.04it/s]\n",
      " 45%|██████████████████████████████▊                                      | 737280/1648877 [00:09<00:08, 106257.24it/s]\n",
      " 46%|███████████████████████████████▉                                     | 761856/1648877 [00:09<00:07, 111131.68it/s]\n",
      " 47%|████████████████████████████████▌                                    | 778240/1648877 [00:09<00:07, 121438.62it/s]\n",
      " 48%|█████████████████████████████████▎                                   | 794624/1648877 [00:10<00:08, 103389.52it/s]\n",
      " 50%|██████████████████████████████████▎                                  | 819200/1648877 [00:10<00:07, 115819.98it/s]\n",
      "9920512it [00:39, 205356.90it/s]                                                                                       \n",
      " 52%|███████████████████████████████████▋                                 | 851968/1648877 [00:10<00:07, 107128.47it/s]\n",
      " 53%|████████████████████████████████████▋                                | 876544/1648877 [00:10<00:07, 110140.68it/s]\n",
      " 55%|█████████████████████████████████████▋                               | 901120/1648877 [00:11<00:06, 114084.07it/s]\n",
      " 56%|██████████████████████████████████████▍                              | 917504/1648877 [00:11<00:06, 121792.87it/s]\n",
      " 57%|███████████████████████████████████████                              | 933888/1648877 [00:11<00:06, 114607.12it/s]\n",
      " 58%|████████████████████████████████████████                             | 958464/1648877 [00:11<00:05, 120965.23it/s]\n",
      " 59%|████████████████████████████████████████▊                            | 974848/1648877 [00:11<00:05, 124596.17it/s]\n",
      " 60%|█████████████████████████████████████████▍                           | 991232/1648877 [00:11<00:05, 114201.32it/s]\n",
      " 62%|█████████████████████████████████████████▉                          | 1015808/1648877 [00:11<00:05, 116591.62it/s]\n",
      " 63%|██████████████████████████████████████████▌                         | 1032192/1648877 [00:12<00:05, 120918.90it/s]\n",
      " 64%|███████████████████████████████████████████▏                        | 1048576/1648877 [00:12<00:04, 122570.73it/s]\n",
      " 65%|███████████████████████████████████████████▉                        | 1064960/1648877 [00:12<00:04, 123524.28it/s]\n",
      " 66%|████████████████████████████████████████████▌                       | 1081344/1648877 [00:12<00:04, 133232.16it/s]\n",
      " 67%|█████████████████████████████████████████████▎                      | 1097728/1648877 [00:12<00:04, 131392.93it/s]\n",
      " 68%|█████████████████████████████████████████████▉                      | 1114112/1648877 [00:12<00:03, 135622.46it/s]\n",
      " 69%|██████████████████████████████████████████████▌                     | 1130496/1648877 [00:12<00:03, 129676.34it/s]\n",
      " 70%|███████████████████████████████████████████████▋                    | 1155072/1648877 [00:12<00:03, 149759.52it/s]\n",
      " 71%|████████████████████████████████████████████████▎                   | 1171456/1648877 [00:13<00:03, 139878.16it/s]\n",
      " 73%|█████████████████████████████████████████████████▎                  | 1196032/1648877 [00:13<00:02, 158467.87it/s]\n",
      " 74%|██████████████████████████████████████████████████▎                 | 1220608/1648877 [00:13<00:02, 166663.02it/s]\n",
      " 76%|███████████████████████████████████████████████████▎                | 1245184/1648877 [00:13<00:02, 166939.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████▋               | 1277952/1648877 [00:13<00:02, 179154.45it/s]\n",
      " 79%|█████████████████████████████████████████████████████▋              | 1302528/1648877 [00:13<00:02, 171292.65it/s]\n",
      " 81%|███████████████████████████████████████████████████████             | 1335296/1648877 [00:13<00:01, 187394.15it/s]\n",
      " 82%|████████████████████████████████████████████████████████            | 1359872/1648877 [00:14<00:01, 192441.39it/s]\n",
      " 84%|█████████████████████████████████████████████████████████▍          | 1392640/1648877 [00:14<00:01, 213332.99it/s]\n",
      " 86%|██████████████████████████████████████████████████████████▍         | 1417216/1648877 [00:14<00:01, 221381.69it/s]\n",
      " 88%|████████████████████████████████████████████████████████████▏       | 1458176/1648877 [00:14<00:00, 243224.98it/s]\n",
      " 90%|█████████████████████████████████████████████████████████████▍      | 1490944/1648877 [00:14<00:00, 254458.49it/s]\n",
      " 92%|██████████████████████████████████████████████████████████████▊     | 1523712/1648877 [00:14<00:00, 259480.35it/s]\n",
      " 94%|████████████████████████████████████████████████████████████████▏   | 1556480/1648877 [00:14<00:00, 260383.21it/s]\n",
      " 97%|█████████████████████████████████████████████████████████████████▉  | 1597440/1648877 [00:14<00:00, 283261.72it/s]\n",
      " 99%|███████████████████████████████████████████████████████████████████▌| 1638400/1648877 [00:14<00:00, 291413.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "  0%|                                                                                         | 0/4542 [00:00<?, ?it/s]\n",
      "\n",
      "8192it [00:00, 15883.37it/s]                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1654784it [00:30, 291413.04it/s]                                                                                       "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "minist = torchvision.datasets.MNIST(root = \"./\", download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.datasets.mnist.MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (from torchtext) (1.2.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from torchtext) (1.12.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchtext) (1.16.4)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from torchtext) (4.32.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading en-ud-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:   0%|                                                             | 0.00/688k [00:00<?, ?B/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:   2%|█▏                                                  | 16.4k/688k [00:00<00:07, 91.2kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:  10%|█████                                                | 65.5k/688k [00:00<00:05, 106kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:  19%|██████████▎                                           | 131k/688k [00:00<00:04, 131kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:  36%|███████████████████▎                                  | 246k/688k [00:00<00:02, 168kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:  74%|███████████████████████████████████████▊              | 508k/688k [00:01<00:00, 224kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip:  81%|███████████████████████████████████████████▋          | 557k/688k [00:01<00:00, 210kB/s]\n",
      "\n",
      ".data\\udpos\\en-ud-v2.zip: 100%|██████████████████████████████████████████████████████| 688k/688k [00:01<00:00, 460kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "WORD = torchtext.data.Field(init_token=\"<bos>\",eos_token=\"<eos>\")\n",
    "UD_TAG = torchtext.data.Field(init_token=\"<bos>\",eos_token=\"<eos>\")\n",
    "train, val, test = torchtext.datasets.UDPOS.splits(fields=(('word', WORD), ('udtag', UD_TAG), (None, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(train.examples[0].word)\n",
    "print(train.examples[0].udtag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速构建自定义数据集\n",
    "用`torch.utils.data`快速构建自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([0, 0, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([0, 0, 1])\n",
    "\n",
    "dataset = Data.TensorDataset(x, y)\n",
    "dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1) tensor(0)\n",
      "tensor(2) tensor(0)\n",
      "tensor(3) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset:\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 细粒度构建自定义数据集\n",
    "继承自torch.utils.data.Dataset,然后重载函数len和getitem来构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['高', '富', '帅', '有'],\n",
       " ['不高', '富', '帅', '有'],\n",
       " ['不高', '不富', '帅', '有'],\n",
       " ['不高', '不富', '不帅', '无']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_1 = ['高','富','帅','有']\n",
    "person_2 = ['不高','富','帅','有']\n",
    "person_3 = ['不高','不富','帅','有']\n",
    "person_4 = ['不高','不富','不帅','无']\n",
    "\n",
    "person = [person_1, person_2, person_3, person_4]\n",
    "\n",
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "class SingleDog(Data.Dataset):\n",
    "    def __init__(self, person):\n",
    "        # 读取文件之类的\n",
    "        self.data = person\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # 获取一个数据；再次可以写预处理；返回一个数据对（x,y）\n",
    "        item = self.data[index]\n",
    "        x = item[:3]    # 前三个\n",
    "        y = item[-1]    # 最后一个\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "single = SingleDog(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数:0, x:['高', '富', '帅'], y:有\n",
      "迭代次数:1, x:['不高', '富', '帅'], y:有\n",
      "迭代次数:2, x:['不高', '不富', '帅'], y:有\n",
      "迭代次数:3, x:['不高', '不富', '不帅'], y:无\n"
     ]
    }
   ],
   "source": [
    "for index, (x, y) in enumerate(single):\n",
    "    print(f\"迭代次数:{index}, x:{x}, y:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集\n",
    "使用torch.utils.data.DataLoader来加载数据集\n",
    "\n",
    "为何要加载？加载后的数据集可以做batch,shuffle,还可以传入一个sample对象告诉是如何抽样的，等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-ff08d242c326>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-ff08d242c326>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    batch_size = batch_size   # 一批数据有几个样例\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensor_person_data_loader = Data.DataLoader(dataset = person_dataset # 要加载哪个数据集\n",
    "                                           batch_size = batch_size   # 一批数据有几个样例\n",
    "                                           shuffle = True            # 需不需要随机洗牌\n",
    "                                           sampler = sampler         # 准备怎么抽样\n",
    "                                           collate_fn = my_collate_fn # 抽出来的样本，准备怎么处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_loader = Data.DataLoader(dataset = single, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数:0, x:[('高', '不高'), ('富', '富'), ('帅', '帅')], y:('有', '有')\n",
      "迭代次数:1, x:[('不高', '不高'), ('不富', '不富'), ('帅', '不帅')], y:('有', '无')\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(single_loader):\n",
    "    x, y = data\n",
    "    print(f\"迭代次数:{index}, x:{x}, y:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候仍不能放到算法里运行，要把汉字变成我们要的tensor，可以使用collate\n",
    "\n",
    "先定义一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {\n",
    "    '高':1,\n",
    "    '富':1,\n",
    "    '帅':1,\n",
    "    '不高':0,\n",
    "    '不富':0,\n",
    "    '不帅':0,\n",
    "    '有':1,\n",
    "    '无':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新建一个数据集（通过字典将汉字转换成数字）\n",
    "class TensorSingleDog(Data.Dataset):\n",
    "    def __init__(self, person):\n",
    "        # 读取文件之类的\n",
    "        self.data = person\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        new_item = []\n",
    "        for feature in item:\n",
    "            new_item.append(word2id[feature])\n",
    "        x = new_item[:3]\n",
    "        y = new_item[-1]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数:0,x:[tensor([1, 0]), tensor([1, 1]), tensor([1, 1])],y:tensor([1, 1])\n",
      "迭代次数:1,x:[tensor([0, 0]), tensor([0, 0]), tensor([1, 0])],y:tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "single = TensorSingleDog(person)\n",
    "single_loader = Data.DataLoader(dataset = single, batch_size = 2)\n",
    "\n",
    "for index, data in enumerate(single_loader):\n",
    "    x, y  = data\n",
    "    print(f\"迭代次数:{index},x:{x},y:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候变成了数字，然而我们传进来的具体的数据应该是一个tensor而非list\n",
    "\n",
    "这时候就使用到`collate`，变为如下：（一个3*2的tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch_data):\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for example in batch_data:\n",
    "        x, y = example\n",
    "        x_batch.append(x)\n",
    "        y_batch.append(y)\n",
    "    x_batch = torch.tensor(x_batch, dtype = torch.float32)\n",
    "    y_batch = torch.tensor(y_batch, dtype = torch.float32)\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_loader = Data.DataLoader(dataset = single, batch_size = 2, collate_fn = my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数:0,x:tensor([[1., 1., 1.],\n",
      "        [0., 1., 1.]]),y:tensor([1., 1.])\n",
      "迭代次数:1,x:tensor([[0., 0., 1.],\n",
      "        [0., 0., 0.]]),y:tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(single_loader):\n",
    "    x, y  = data\n",
    "    print(f\"迭代次数:{index},x:{x},y:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，数据预处理阶段完毕，这才是我们要输入到模型里面的x，y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、构建模型\n",
    "torchvision下面有常用的计算机视觉用到的模型，而没有nlp的模型（nlp没有统一的模型）\n",
    "\n",
    "使用torch.hub.load()，来加载别人的模型，然而需要写github的人在根目录写一个配置文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速构建自己的神经网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 10),\n",
    "    torch.nn.ReLU(inplace=True),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")\n",
    "my_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1017],\n",
       "        [0.6189],\n",
       "        [0.4371],\n",
       "        [0.1458],\n",
       "        [0.0999],\n",
       "        [1.1759],\n",
       "        [0.1633],\n",
       "        [0.1000],\n",
       "        [0.1118],\n",
       "        [0.1082]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_net((torch.randn(10, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 快速构建多层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True),\n",
       " Linear(in_features=10, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hun_layer = [torch.nn.Linear(10, 10) for _ in range(100)]\n",
    "hun_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上我们用的都是现成的，没有做很好的处理，要想这样，看下面："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 深度自定义\n",
    "想深度自定义（你的模型和别的模型不一样），可以去继承nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SingleDogCls(nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(SingleDogCls, self).__init__()\n",
    "        \n",
    "        self.hidden = nn.Linear(n_feature, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.predict = nn.Linear(n_hidden, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.predict(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['高', '富', '帅', '有'],\n",
       " ['不高', '富', '帅', '有'],\n",
       " ['不高', '不富', '帅', '有'],\n",
       " ['不高', '不富', '不帅', '无']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "will_u_be_single = SingleDogCls(n_feature = 3, n_hidden = 10, n_output = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleDogCls(\n",
       "  (hidden): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will_u_be_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hiddenlayer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-37e0f215637b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 画出神经网络结构\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mhiddenlayer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwill_u_be_single\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hiddenlayer'"
     ]
    }
   ],
   "source": [
    "# 画出神经网络结构\n",
    "import hiddenlayer as hl\n",
    "hl.build_graph(will_u_be_single, torch.randn([10, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数：0\n",
      "data:(tensor([[1., 1., 1.],\n",
      "        [0., 1., 1.]]), tensor([1., 1.]))\n",
      "tensor([[-0.5101],\n",
      "        [-0.3515]], grad_fn=<AddmmBackward>) tensor([1., 1.])\n",
      "迭代次数：1\n",
      "data:(tensor([[0., 0., 1.],\n",
      "        [0., 0., 0.]]), tensor([1., 0.]))\n",
      "tensor([[-0.5553],\n",
      "        [-0.4379]], grad_fn=<AddmmBackward>) tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 迭代去计算\n",
    "for index, data in enumerate(single_loader):\n",
    "    print(f\"迭代次数：{index}\")\n",
    "    print(f\"data:{data}\")\n",
    "    input_x, ground_truth = data\n",
    "    predict = will_u_be_single(input_x)\n",
    "    print(predict, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、定义损失\n",
    "度量，模型算出来的predict，和你的ground_truth之间的差距\n",
    "\n",
    "一般用损失函数来度量，损失函数及其重要，如果度量的不准，那么整个模型学习的方向就会有偏差。官方定义的损失函数在目前有两个地方：\n",
    "- torch.nn\n",
    "- torch.nn.functional\n",
    "\n",
    "torch.nn下面的loss函数，是作为模型的一部分存在的，实际上是torch.nn.funcitonal下loss函数的封装，实际上还是调用了后者下的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(2.)\n",
    "b = torch.tensor(5.)\n",
    "\n",
    "F.l1_loss(a, b) # |a - b|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(a, b) # 均方误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的两个数列之间的距离，可以用l1或者mse搞定，如果是两个分布，怎么度量呢？两个图像？两个类别？\n",
    "\n",
    "torch.nn.functional下面，有这么多的损失函数：足够日常使用了\n",
    "\n",
    "但是，如果你需要的loss，官方没有，你有两个方法实现自己的loss funtion\n",
    "- 继承torch.nn.module实现loss\n",
    "- 继承torch.autograd.funtion 实现loss，区别在于，你想把这个loss当作什么（模型还是函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "继承"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、实现优化算法\n",
    "Pytorch集成了常见的优化算法，包括：\n",
    "- torch.optim.SGD..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "optimizer = torch.optim.SGD(will_u_be_single.parameters(), lr = 0.05, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你需要自定义优化算法，也可以继承torch.optim.Optimizer，通过实现其中的部分方法如step等，来优化模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyOptimizer(Optimizer):\n",
    "    def step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调整学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd：自动求导机制\n",
    "PyTorch 中所有神经网络的核心是 autograd 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。\n",
    "\n",
    "autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor是这个包的核心类，如果设置`.requires_grad`为True，那么将跟踪所有对于该张量的操作。当完成计算后通过调用`.backward()`，自动计算所有的梯度，这个张量的所有梯度将会自动累加到`.grad`属性\n",
    "\n",
    "在自动梯度计算中还有另一个重要的类Function。\n",
    "Tensor与Function互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个`.grad_fn`属性，这个属性引用了一个创建了Tensor的Function（除非这个张量是用户手动创建的，这时这个张量的`grad_fn`为None\n",
    "\n",
    "如果需要计算导数，可以在Tensor上调用`.backward()`。如果Tensor是一个标量（即它包含一个元素数据）则不需要为`backward()`指定任何参数，但是如果它有更多的元素，==需要指定一个`gradient`参数来匹配张量的形状==。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个张量，并追踪它的计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对张量进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果y已经被计算出来了，所以，`grad_fn`已经被自动生成了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001B904261E48>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad()`可以改变现有张量的`requires_grad`属性。如果没有指定的话，默认输入的flag是False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001B904261A20>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度\n",
    "反向传播，因为out是一个标量(scalar)，out.backward()等价于out.backward(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "# print gradients d(out)/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==下面不太懂==\n",
    "$out = \\frac{1}{4}\\sum_{i}z_i, z_i = 3(x_i+2)^2 and z_i|_{x_i=1}=27$\n",
    "因此,$\\frac{\\partial o}{\\partial x_i}=\\frac{3}{2}(x_i+2),hence\\frac{\\partial o}{\\partial x_i}|_{x_i=1}=\\frac{9}{2}=4.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用autograd做更多的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-534.7850,  -32.8364, 1520.0532], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果，`.requires_grad=True`而你又不希望进行autograd的计算，那么可以将变量包裹在`with torch.no_grad()`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑难解答\n",
    "----\n",
    "----\n",
    "1.requires_grad\n",
    "\n",
    "若一个节点的被设置为True，则所有依赖它的节点的都为True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "2.`.backward()`\n",
    "以下参考自：[PyTorch中的自动求导backward()所需参数含义](https://www.cnblogs.com/JeasonIsCoding/p/10164948.html)\n",
    "如果out.backward()中的out是一个标量的话（相当于神经网络有一个样本，这个样本有两个属性，神经网络有一个输出），那么此时我的backward函数是不需要任何输入参数的。\n",
    "如果out是一个向量（或者理解为1*N的矩阵）的话，我们传入的参数，是对原来模型正常求导出来的雅可比矩阵进行线性操作，可以把我们传进的参数看成是一个行向量，那么我们得到的结果为：\n",
    "$$\n",
    "(arg \\times Jacobian)\n",
    "$$\n",
    "![](https://raw.githubusercontent.com/CallMeNumber1/FigureBed/windows/img/20190917203500.png)\n",
    "\n",
    "总结，因为经过了复杂的神经网络之后，out中每个数值都是由很多输入样本的属性线性或非线性组合而成的，那么out中的每个数值和输入数据中的每个数值都有关联，也就是说out中的每个数可以对a中每个数求导（即得到雅可比矩阵），那么我们backward()的参数[k1,k2,...,kn]的含义就是：\n",
    "![](https://raw.githubusercontent.com/CallMeNumber1/FigureBed/windows/img/20190917203550.png)\n",
    "也可以理解为每个out分量对$a_n$求导时的权重（对$a_i$的权重即为$k_i$）\n",
    "\n",
    "也可以扩展一下多个样本的分类问题，k的维度应该是`输入样本的个数*分类的个数`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.mm(a, b)    # 矩阵a和矩阵b相乘\n",
    "\n",
    "torch.mul(a, b)   # 矩阵a和矩阵b中对应元素相乘（即要求a和b维度相同）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
